{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Nova Customization SDK - Quick Start Guide\n",
    "\n",
    "This notebook provides a basic walkthrough of the Amazon Nova Customization SDK for fine-tuning Nova models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Loading and preparing datasets\n",
    "2. Fine-tuning a Nova model with SFT (Supervised Fine-Tuning)\n",
    "3. Monitoring training progress\n",
    "4. Deploying your model\n",
    "5. Using other training methods to customize your Nova model\n",
    "\n",
    "## Table of Contents\n",
    "- [Step 1: Import Required Modules](#step-1-import-required-modules)\n",
    "- [Step 2: Configure Your AWS Resources](#step-2-configure-your-aws-resources)\n",
    "- [Step 3: Prepare your Dataset](#step-3-prepare-your-dataset)\n",
    "- [Step 4: Configure Runtime Infrastructure](#step-4-configure-runtime-infrastructure)\n",
    "- [Step 5: Initialize Nova Model Customizer](#step-5-initialize-nova-model-customizer)\n",
    "  - [Step 5.1 Data Mixing Configuration (Optional)](#51-data-mixing-configuration-optional)\n",
    "- [Step 6: Start Training](#step-6-start-training)\n",
    "- [Step 7: Monitor Training Progress](#step-7-monitor-training-progress)\n",
    "- [Step 8: Evaluate the custom Model (After Training Completes)](#step-8-evaluate-the-custom-model-after-training-completes)\n",
    "- [Step 9: Deploy Your Model (After Training Completes)](#step-9-deploy-your-model-after-training-completes)\n",
    "- [Try Additional Training Methods (Optional)](#try-additional-training-methods-optional)\n",
    "- [Summary](#summary)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured\n",
    "- S3 bucket for data and model artifacts\n",
    "- IAM permissions for SageMaker and Bedrock\n",
    "- Nova Customization SDK installed per [its README](https://github.com/aws-samples/sample-nova-customization-sdk?tab=readme-ov-file#installation)\n",
    "- (If using the SMHP runtime below) Correct version of Sagemaker HyperPod CLI installed; see the SDK README for details\n",
    "\n",
    "## Helpful Links\n",
    "- If you haven't already, please take a look at the `docs/spec.md` file for more information on what parameters you can change in the code below.\n",
    "- Also visit the `README.md` for a high-level overview of the Nova SDK and its capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../ && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError, ProfileNotFound\n",
    "\n",
    "\n",
    "def load_credentials(profile=None):\n",
    "    \"\"\"\n",
    "    Load AWS credentials with fallback behavior.\n",
    "\n",
    "    Args:\n",
    "        profile (str, optional): AWS profile name. If provided, loads from credentials file.\n",
    "                               If None, uses current authenticated AWS session.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing AWS credentials and region\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If credential loading fails\n",
    "    \"\"\"\n",
    "    if profile:\n",
    "        # Try loading from credentials file\n",
    "        try:\n",
    "            session = boto3.Session(profile_name=profile)\n",
    "            credentials = session.get_credentials()\n",
    "\n",
    "            if not credentials:\n",
    "                raise RuntimeError(f\"No credentials found for profile '{profile}'\")\n",
    "\n",
    "        except ProfileNotFound:\n",
    "            raise RuntimeError(f\"Profile '{profile}' not found in credentials file\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load credentials from file: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Try loading from current authenticated session\n",
    "        try:\n",
    "            session = boto3.Session()\n",
    "            credentials = session.get_credentials()\n",
    "\n",
    "            if not credentials:\n",
    "                raise RuntimeError(\"No credentials found in current AWS session\")\n",
    "\n",
    "        except NoCredentialsError:\n",
    "            raise RuntimeError(\"No AWS credentials configured\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load credentials from current session: {e}\")\n",
    "\n",
    "        # Validate credentials by making a test call\n",
    "    try:\n",
    "        sts_client = session.client(\"sts\")\n",
    "        sts_client.get_caller_identity()\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"Invalid AWS credentials: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to validate credentials: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"aws_access_key_id\": credentials.access_key,\n",
    "        \"aws_secret_access_key\": credentials.secret_key,\n",
    "        \"aws_session_token\": credentials.token,\n",
    "        \"region_name\": session.region_name or \"us-east-1\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = load_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core import\n",
    "from amzn_nova_customization_sdk import *\n",
    "\n",
    "print(\"‚úÖ SDK imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Your AWS Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these values for your environment\n",
    "S3_BUCKET = \"nova-customization-beta\"  # TODO: Replace with your S3 bucket\n",
    "S3_DATA_PATH = f\"s3://{S3_BUCKET}/demo/input\"\n",
    "S3_OUTPUT_PATH = f\"s3://{S3_BUCKET}/demo/output\"\n",
    "\n",
    "print(f\"Data Path: {S3_DATA_PATH}\")\n",
    "print(f\"Output Path: {S3_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Your Dataset\n",
    "\n",
    "The SDK supports three formats: **JSONL**, **JSON**, and **CSV**. This example uses JSONL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample training data\n",
    "import json\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"answer\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain what AWS is.\",\n",
    "        \"answer\": \"AWS (Amazon Web Services) is a comprehensive cloud computing platform that provides on-demand computing resources and services.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is Python used for?\",\n",
    "        \"answer\": \"Python is a versatile programming language used for web development, data analysis, artificial intelligence, scientific computing, and automation.\",\n",
    "    },\n",
    "] * 100\n",
    "\n",
    "# Save sample data locally\n",
    "with open(\"training_data.jsonl\", \"w\") as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Sample data created: training_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Transform, and Validate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset loader\n",
    "loader = JSONLDatasetLoader(\n",
    "    question=\"question\",  # Column name for questions in your data\n",
    "    answer=\"answer\",  # Column name for answers in your data\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "loader.load(\"training_data.jsonl\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nüìä Dataset Preview:\")\n",
    "loader.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data for Nova model training\n",
    "loader.transform(method=TrainingMethod.SFT_LORA, model=Model.NOVA_LITE_2)\n",
    "\n",
    "print(\"‚úÖ Data transformed to Converse format\")\n",
    "print(\"\\nüìä Transformed Data Preview:\")\n",
    "\n",
    "loader.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates transformed data for the method and model combination. Prints out a \"Validation completed\" message if successful.\n",
    "loader.validate(method=TrainingMethod.SFT_LORA, model=Model.NOVA_LITE_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation sets\n",
    "train_loader, val_loader, _ = loader.split_data(\n",
    "    train_ratio=0.7, val_ratio=0.2, test_ratio=0.1\n",
    ")\n",
    "\n",
    "# Save datasets\n",
    "# For production, upload to S3:\n",
    "train_path = train_loader.save_data(f\"{S3_DATA_PATH}/train.jsonl\")\n",
    "val_path = val_loader.save_data(f\"{S3_DATA_PATH}/val.jsonl\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training data saved to: {train_path}\")\n",
    "print(f\"‚úÖ Validation data saved to: {val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Runtime Infrastructure\n",
    "\n",
    "Choose between:\n",
    "- **SMTJRuntimeManager**: For SageMaker Training Jobs\n",
    "- **SMHPRuntimeManager**: For SageMaker HyperPod clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_customization_sdk.model.model_enums import Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: SageMaker Training Jobs (SMTJ)\n",
    "runtime = SMTJRuntimeManager(\n",
    "    instance_type=\"ml.p5.48xlarge\",  # Choose appropriate instance\n",
    "    instance_count=4,  # Number of instances\n",
    "    # execution_role=\"<your execution role>\",  # TODO: Choose execution role (if different from current role)\n",
    ")\n",
    "\n",
    "platform = Platform.SMTJ\n",
    "\n",
    "print(\"‚úÖ Runtime configured for SageMaker Training Jobs\")\n",
    "print(f\"   Instance Type: {runtime.instance_type}\")\n",
    "print(f\"   Instance Count: {runtime.instance_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: SageMaker HyperPod (if using HyperPod cluster)\n",
    "# Uncomment and configure if using HyperPod:\n",
    "\n",
    "# runtime = SMHPRuntimeManager(\n",
    "#     instance_type=\"ml.p5.48xlarge\",\n",
    "#     instance_count=4,\n",
    "#     cluster_name=\"your-cluster-name\",\n",
    "#     namespace=\"your-namespace\"\n",
    "# )\n",
    "#\n",
    "# platform = Platform.SMHP\n",
    "#\n",
    "# print(\"‚úÖ Runtime configured for SageMaker HyperPod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to trigger Hyperpod job you will need to add the nemo_launcher to PYTHONPATH\n",
    "\n",
    "# import os\n",
    "\n",
    "# hyperpod_clone_path = <path where you cloned the hyperpod repo>\n",
    "# os.environ['PYTHONPATH'] = f'{hyperpod_clone_path}/src/hyperpod_cli/sagemaker_hyperpod_recipes/launcher/nemo/nemo_framework_launcher/launcher_scripts:' + os.environ.get('PYTHONPATH', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Nova Model Customizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MLFlow monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLflow monitor to monitor metrics, this is optional\n",
    "mlflow_monitor = MLflowMonitor(\n",
    "    tracking_uri=\"<Mlflow App/Server Arn>\",\n",
    "    experiment_name=\"nova-customization-experiment\",  # replace with experiment name\n",
    "    run_name=\"nova-lite2-sft-run-1\",  # replace with run name\n",
    ")\n",
    "\n",
    "# mlflow_monitor = MLflowMonitor() # uses default mlflow app, if it exists\n",
    "\n",
    "# mlflow_monitor = MLflowMonitor(\n",
    "#     experiment_name=\"nova-customization-experiment\", # replace with experiment name\n",
    "#     run_name=\"nova-lite2-sft-run-1\" # replace with run name\n",
    "# ) # uses default mlflow app, if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customizer\n",
    "customizer = NovaModelCustomizer(\n",
    "    model=Model.NOVA_LITE_2,  # Choose your Nova model\n",
    "    method=TrainingMethod.SFT_LORA,  # Training method\n",
    "    infra=runtime,  # Runtime configuration\n",
    "    data_s3_path=train_path,  # Training data path\n",
    "    output_s3_path=S3_OUTPUT_PATH,  # Output path for artifacts\n",
    "    mlflow_monitor=mlflow_monitor,  # optional\n",
    ")\n",
    "print(\"‚úÖ NovaModelCustomizer initialized\")\n",
    "print(f\"   Model: Nova Lite 2.0\")\n",
    "print(f\"   Method: SFT with LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Data Mixing Configuration (Optional)\n",
    "\n",
    "Data mixing is a Nova Forge feature that allows you to blend your custom data with Nova's curated datasets. This can improve model performance by maintaining general capabilities while adding domain-specific knowledge.\n",
    "\n",
    "When data mixing is enabled:\n",
    "- Your custom data and Nova's curated data are mixed at specified percentages\n",
    "- The sum of Nova data percentages must equal 100%\n",
    "- Customer data can range from 0-100%\n",
    "- If customer data is 100%, no Nova data is used and all nova data should sum to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Enable data mixing with 50% customer data and 50% Nova data\n",
    "# Create customizer\n",
    "customizer_with_mixing = NovaModelCustomizer(\n",
    "    model=Model.NOVA_LITE_2,  # Choose your Nova model\n",
    "    method=TrainingMethod.SFT_LORA,  # Training method\n",
    "    infra=runtime,  # Runtime configuration\n",
    "    data_s3_path=train_path,  # Training data path\n",
    "    output_s3_path=S3_OUTPUT_PATH,  # Output path for artifacts\n",
    "    mlflow_monitor=mlflow_monitor,  # optional\n",
    "    data_mixing_enabled=True,\n",
    ")\n",
    "\n",
    "# Check default data mixing configuration\n",
    "customizer_with_mixing.get_data_mixing_config()\n",
    "\"\"\"\n",
    "{\n",
    "    \"customer_data_percent\": 50,\n",
    "    \"nova_code_percent\": 1.0,\n",
    "    \"nova_general_percent\": 0.10,\n",
    "    ......\n",
    "    ......\n",
    "    \"nova_chat_percent\": 50\n",
    "    # all Nova fields sum to 100\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Overwrite the data mixing percentages in default config\n",
    "customizer_with_mixing.set_data_mixing_config(\n",
    "    {\n",
    "        \"customer_data_percent\": 50,  # 50% customer data\n",
    "        \"nova_code_percent\": 30,  # 30% Nova code data\n",
    "        \"nova_general_percent\": 70,  # 70% Nova general data\n",
    "        # rest all nova fields are made 0\n",
    "        # Nova percentages must sum to 100%\n",
    "    }\n",
    ")\n",
    "\n",
    "# Verify your updates\n",
    "customizer_with_mixing.get_data_mixing_config()\n",
    "\"\"\"\n",
    "{\n",
    "    \"customer_data_percent\": 50,\n",
    "    \"nova_code_percent\": 30,\n",
    "    \"nova_general_percent\": 70,\n",
    "    # rest all nova fields are 0\n",
    "}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters - You can edit these by navigating to the Nova Customization public documentation, linked in the ../docs/spec.md document.\n",
    "training_config = {\n",
    "    \"lr\": 5e-6,  # Learning rate\n",
    "    \"warmup_steps\": 100,  # Warmup steps\n",
    "    \"global_batch_size\": 64,  # Batch size\n",
    "    \"max_length\": 8192,  # Max sequence length\n",
    "}\n",
    "\n",
    "# Start training\n",
    "training_result = customizer.train(\n",
    "    job_name=\"nova-quickstart-training-nova-2\", overrides=training_config\n",
    ")\n",
    "\n",
    "# \"Dry Run\" mode is also supported. This feature is useful whenever you want to test or validate inputs and still have a recipe generated, without starting a job.\n",
    "# customizer.train(\n",
    "#     job_name=\"nova-quickstart-training-dry-run\",\n",
    "#     dry_run=True, <-- Set dry_run parameter\n",
    "#     overrides=training_config\n",
    "# )\n",
    "\n",
    "print(\"\\nüöÄ Training job started!\")\n",
    "print(training_result)\n",
    "print(\n",
    "    f\"   üìç Checkpoint URI where the model will be saved: {training_result.model_artifacts.checkpoint_s3_path}\"\n",
    ")\n",
    "print(f\"   üÜî Job ID: {training_result.job_id}\")\n",
    "print(f\"   üìÇ Output Path: {training_result.model_artifacts.output_s3_path}\")\n",
    "\n",
    "# Save job ID for later\n",
    "job_id = training_result.job_id\n",
    "escrow_uri = training_result.model_artifacts.checkpoint_s3_path\n",
    "output_path = training_result.model_artifacts.output_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Monitor Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) While training is ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View recent training logs\n",
    "print(\"üìã Training Logs:\")\n",
    "print(\"=\" * 80)\n",
    "customizer.get_logs(limit=50, start_from_head=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) After Training is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = training_result.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = CloudWatchLogMonitor.from_job_id(job_id=job_id, platform=platform)\n",
    "monitor.show_logs(limit=100, start_from_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate the custom Model (After Training Completes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation jobs allow you to test your customized model against pre-set or custom benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these values for your environment\n",
    "S3_BUCKET = S3_BUCKET\n",
    "S3_DATA_PATH = f\"s3://{S3_BUCKET}/demo/input\"\n",
    "S3_OUTPUT_PATH = f\"s3://{S3_BUCKET}/demo/output\"\n",
    "\n",
    "infra = SMTJRuntimeManager(\n",
    "    instance_type=\"ml.p5.48xlarge\",  # Change the instance type if needed (e.g. p5.48xlarge)\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "evaluator = NovaModelCustomizer(\n",
    "    model=Model.NOVA_LITE_2,  # You can also use your trained model here for eval\n",
    "    method=TrainingMethod.EVALUATION,\n",
    "    infra=infra,\n",
    "    data_s3_path=S3_DATA_PATH,  # The data_s3_path is not used in eval job\n",
    "    output_s3_path=S3_OUTPUT_PATH,  # This will be your eval output path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation can be 3 dimensional \n",
    "- Using Public Benchmark to check on Models generalizability is maintained or not.\n",
    "- Using your custom Data to validate models performance on YOUR tasks.\n",
    "- Using LLM As Judge in domains where response quality is hard to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_eval_result = evaluator.evaluate(\n",
    "    job_name=\"eval-test-mmlu\",  # The job name you specified\n",
    "    eval_task=EvaluationTask.MMLU,  # The eval task\n",
    ")\n",
    "\n",
    "byod_eval_result = evaluator.evaluate(\n",
    "    job_name=\"eval-test-byod\",\n",
    "    eval_task=EvaluationTask.GEN_QA,\n",
    "    data_s3_path=\"s3://<data-s3-bucket>/nova-customization/gen_qa.jsonl\",  # TODO: Replace with your data path\n",
    "    # model_path='s3://customer-escrow-<your-model-ckpt-bucket>/your-model-path/' # TODO: Replace with your model path\n",
    "    overrides={\"max_new_tokens\": 2048},\n",
    ")\n",
    "\n",
    "# byom_eval_result = evaluator.evaluate(\n",
    "#     job_name='eval-test-byom',\n",
    "#     eval_task=EvaluationTask.GEN_QA,\n",
    "#     data_s3_path='s3://<your-byom-dataset-bucket>/input/eval/byom/byom_data.jsonl', # TODO: Replace with your dataset\n",
    "#     processor={\n",
    "#         \"lambda_arn\": \"arn:aws:lambda:<region>:<account>:function:<lambda>\" # TODO: Your byom lambda\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# llm_judge_eval_result = evaluator.evaluate(\n",
    "#     job_name='eval-test-llm-judge',\n",
    "#     eval_task=EvaluationTask.LLM_JUDGE,\n",
    "#     data_s3_path='s3://<your-llm-judge-dataset-bucket>/input/eval/llm_judge/llm_judge.jsonl' # TODO: Replace with your dataset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  üìç Bring Your Own Data Job ID: \", byod_eval_result.job_id)\n",
    "print(\"  üìÇ Bring Your Own Data Output Path:\", byod_eval_result.eval_output_path)\n",
    "print(\"  üìç MMLU Job ID:\", mmlu_eval_result.eval_output_path)\n",
    "print(\"  üìÇ MMLU Output Path:\", mmlu_eval_result.eval_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View recent training logs\n",
    "\n",
    "print(\"üìã Evaluation Job Logs:\")\n",
    "print(\"=\" * 80)\n",
    "evaluator.get_logs(limit=50, start_from_head=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Deploy Your Model (After Training Completes)\n",
    "\n",
    "Once training is complete, deploy your model to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model artifacts path from training result\n",
    "# After training completes, use:\n",
    "\n",
    "# Deploy to Bedrock On-Demand\n",
    "deployment_result = customizer.deploy(\n",
    "    endpoint_name=\"my-custom-nova-model\", job_result=training_result\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Model deployment started!\")\n",
    "print(f\"   Endpoint Name: {deployment_result.endpoint.endpoint_name}\")\n",
    "print(f\"   Status: {deployment_result.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Additional Training Methods (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Pre-Training (CPT)\n",
    "* CPT is another training technique offered for Nova model customization.\n",
    "* Expand this section for example code!\n",
    "* More information can be found here: [AWS Docs on CPT](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-cpt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Creation\n",
    "import json\n",
    "\n",
    "cpt_sample_data = [{\"text\": \"AWS stands for Amazon Web Services\"}] * 100\n",
    "\n",
    "# Save sample data locally\n",
    "with open(\"cpt_training_data.jsonl\", \"w\") as f:\n",
    "    for item in cpt_sample_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Save Data to s3\n",
    "loader = JSONLDatasetLoader(\n",
    "    text=\"text\",  # Column name for text in your data\n",
    ")\n",
    "\n",
    "# Load and save the data to s3.\n",
    "loader.load(\"cpt_training_data.jsonl\")\n",
    "train_path = loader.save_data(\n",
    "    f\"{S3_DATA_PATH}/train.jsonl\"\n",
    ")  # S3_DATA_PATH is set up in Step 2 of the SFT example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Infrastructure Setup\n",
    "runtime = SMHPRuntimeManager(\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    instance_count=4,\n",
    "    cluster_name=\"your-cluster-name\",\n",
    "    namespace=\"your-namespace\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Runtime configured for SageMaker HyperPod\")\n",
    "print(f\"   Instance Type: {runtime.instance_type}\")\n",
    "print(f\"   Instance Count: {runtime.instance_count}\")\n",
    "\n",
    "# Step 4: Create customizer\n",
    "customizer = NovaModelCustomizer(\n",
    "    model=Model.NOVA_LITE_2,  # Choose your Nova model\n",
    "    method=TrainingMethod.CPT,  # Training method\n",
    "    infra=runtime,  # Runtime configuration\n",
    "    data_s3_path=train_path,  # Training data path\n",
    "    output_s3_path=S3_OUTPUT_PATH,  # Output path for artifacts, set up in Step 2 of the SFT example.\n",
    ")\n",
    "print(\"‚úÖ NovaModelCustomizer initialized\")\n",
    "print(f\"   Model: Nova Lite 2.0\")\n",
    "print(f\"   Method: CPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Training\n",
    "\n",
    "# Define training hyperparameters\n",
    "training_config = {\n",
    "    \"lr\": 5e-6,  # Learning rate\n",
    "    \"warmup_steps\": 100,  # Warmup steps\n",
    "    \"global_batch_size\": 64,  # Batch size\n",
    "    \"max_length\": 8192,  # Max sequence length\n",
    "}\n",
    "\n",
    "# Start training\n",
    "training_result = customizer.train(\n",
    "    job_name=\"cpt-quickstart-training-nova-2\", overrides=training_config\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training job started!\")\n",
    "print(training_result)\n",
    "print(\n",
    "    f\"   üìç Checkpoint URI where the model will be saved: {training_result.model_artifacts.checkpoint_s3_path}\"\n",
    ")\n",
    "print(f\"   üÜî Job ID: {training_result.job_id}\")\n",
    "print(f\"   üìÇ Output Path: {training_result.model_artifacts.output_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization (DPO)\n",
    "\n",
    "- DPO is another fine-tuning method that can be used to train models. Below is an example dataset you can download and the commands necessary to run it.\n",
    "- Expand this section for example code!\n",
    "- More information can be found here: [AWS Docs on DPO](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-dpo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Creation\n",
    "import json\n",
    "\n",
    "dpo_sample_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": \"Question: You are configuring an AWS application that needs to handle increased traffic. If scaling horizontally adds more instances to distribute load, what does scaling vertically do to handle increased demand?\"\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"candidates\": [\n",
    "                    {\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"text\": \"Scaling vertically increases the resources (CPU, memory, storage) of existing instances to handle more load.\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"preferenceLabel\": \"preferred\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"text\": \"Scaling vertically distributes the workload across multiple availability zones.\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"preferenceLabel\": \"non-preferred\",\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "] * 100\n",
    "\n",
    "# Save sample data locally\n",
    "with open(\"dpo_training_data.jsonl\", \"w\") as f:\n",
    "    for item in dpo_sample_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Save Data to s3\n",
    "loader = JSONLDatasetLoader()  # DPO dataset transformation isn't supported yet, so column mappings don't need to be provided here.\n",
    "\n",
    "# Load and save the data to s3.\n",
    "loader.load(\"dpo_training_data.jsonl\")\n",
    "train_path = loader.save_data(\n",
    "    f\"{S3_DATA_PATH}/dpo_train.jsonl\"\n",
    ")  # S3_DATA_PATH is set up in Step 2 of the SFT example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Infrastructure Setup\n",
    "runtime = SMHPRuntimeManager(\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    instance_count=2,\n",
    "    cluster_name=\"your-cluster-name\",\n",
    "    namespace=\"your-namespace\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Runtime configured for SageMaker HyperPod\")\n",
    "print(f\"   Instance Type: {runtime.instance_type}\")\n",
    "print(f\"   Instance Count: {runtime.instance_count}\")\n",
    "\n",
    "# Step 4: Create customizer\n",
    "customizer = NovaModelCustomizer(\n",
    "    model=Model.NOVA_MICRO,  # Choose your Nova model\n",
    "    method=TrainingMethod.DPO_LORA,  # Training method\n",
    "    infra=runtime,  # Runtime configuration\n",
    "    data_s3_path=train_path,  # Training data path\n",
    "    output_s3_path=S3_OUTPUT_PATH,  # Output path for artifacts, set up in Step 2 of the SFT example.\n",
    ")\n",
    "print(\"‚úÖ NovaModelCustomizer initialized\")\n",
    "print(f\"   Model: Nova Micro\")\n",
    "print(f\"   Method: DPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Training\n",
    "\n",
    "# Define training hyperparameters\n",
    "training_config = {\n",
    "    \"lr\": 1e-6,  # Learning rate\n",
    "    \"warmup_steps\": 10,  # Warmup steps\n",
    "    \"max_length\": 4096,  # Max sequence length\n",
    "}\n",
    "\n",
    "# Start training\n",
    "training_result = customizer.train(\n",
    "    job_name=\"dpo-quickstart-training-nova\", overrides=training_config\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training job started!\")\n",
    "print(training_result)\n",
    "print(\n",
    "    f\"   üìç Checkpoint URI where the model will be saved: {training_result.model_artifacts.checkpoint_s3_path}\"\n",
    ")\n",
    "print(f\"   üÜî Job ID: {training_result.job_id}\")\n",
    "print(f\"   üìÇ Output Path: {training_result.model_artifacts.output_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've completed the basic workflow:\n",
    "\n",
    "‚úÖ **Loaded and prepared data** in JSONL format  \n",
    "‚úÖ **Transformed data** to Nova's Converse format  \n",
    "‚úÖ **Configured runtime** infrastructure (SMTJ or HyperPod)  \n",
    "‚úÖ **Started training** with custom hyperparameters  \n",
    "‚úÖ **Monitored progress** via logs  \n",
    "‚úÖ **Deployed the model** to Amazon Bedrock  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Use your own data**: Replace the sample data with your training dataset\n",
    "- **Tune hyperparameters**: Adjust learning rate, batch size, epochs, etc.\n",
    "- **Evaluate performance**: Use `customizer.evaluate()` to benchmark your model\n",
    "- **Run batch inference**: Process large datasets with `customizer.batch_inference()`\n",
    "\n",
    "## Available Models\n",
    "\n",
    "- `Model.NOVA_MICRO`\n",
    "- `Model.NOVA_LITE`\n",
    "- `Model.NOVA_PRO`\n",
    "- `Model.NOVA_LITE_2`\n",
    "\n",
    "## Available Training Methods\n",
    "\n",
    "- `TrainingMethod.CPT` - Continued Pre-Training\n",
    "- `TrainingMethod.DPO_LORA` - Direct Preference Optimization with LoRA\n",
    "- `TrainingMethod.DPO_FULL` - Full Direct Preference Optimization\n",
    "- `TrainingMethod.SFT_LORA` - Supervised Fine-Tuning with LoRA\n",
    "- `TrainingMethod.SFT_FULL` - Full supervised fine-tuning\n",
    "- `TrainingMethod.RFT_LORA` - Reinforcement Fine-Tuning with LoRA\n",
    "- `TrainingMethod.RFT_FULL` - Full reinforcement Fine-Tuning\n",
    "- `TrainingMethod.Evaluation` - Model evaluation\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Amazon Nova Models](https://aws.amazon.com/bedrock/nova/)\n",
    "- [SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/)\n",
    "- [Amazon Bedrock](https://aws.amazon.com/bedrock/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
