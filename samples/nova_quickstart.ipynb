{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Nova Customization SDK - Quick Start Guide\n",
    "\n",
    "This notebook provides a basic walkthrough of the Amazon Nova Customization SDK for fine-tuning Nova models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Loading and preparing datasets\n",
    "2. Fine-tuning a Nova model with SFT (Supervised Fine-Tuning)\n",
    "3. Monitoring training progress\n",
    "4. Deploying your model\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured\n",
    "- S3 bucket for data and model artifacts\n",
    "- IAM permissions for SageMaker and Bedrock\n",
    "- Nova Customization SDK installed per [its README](https://github.com/aws-samples/sample-nova-customization-sdk?tab=readme-ov-file#installation)\n",
    "- (If using the SMHP runtime below) Correct version of Sagemaker HyperPod CLI installed; see the SDK README for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../ && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError, ProfileNotFound\n",
    "\n",
    "\n",
    "def load_credentials(profile=None):\n",
    "    \"\"\"\n",
    "    Load AWS credentials with fallback behavior.\n",
    "\n",
    "    Args:\n",
    "        profile (str, optional): AWS profile name. If provided, loads from credentials file.\n",
    "                               If None, uses current authenticated AWS session.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing AWS credentials and region\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If credential loading fails\n",
    "    \"\"\"\n",
    "    if profile:\n",
    "        # Try loading from credentials file\n",
    "        try:\n",
    "            session = boto3.Session(profile_name=profile)\n",
    "            credentials = session.get_credentials()\n",
    "\n",
    "            if not credentials:\n",
    "                raise RuntimeError(f\"No credentials found for profile '{profile}'\")\n",
    "\n",
    "        except ProfileNotFound:\n",
    "            raise RuntimeError(f\"Profile '{profile}' not found in credentials file\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load credentials from file: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Try loading from current authenticated session\n",
    "        try:\n",
    "            session = boto3.Session()\n",
    "            credentials = session.get_credentials()\n",
    "\n",
    "            if not credentials:\n",
    "                raise RuntimeError(\"No credentials found in current AWS session\")\n",
    "\n",
    "        except NoCredentialsError:\n",
    "            raise RuntimeError(\"No AWS credentials configured\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load credentials from current session: {e}\")\n",
    "\n",
    "        # Validate credentials by making a test call\n",
    "    try:\n",
    "        sts_client = session.client(\"sts\")\n",
    "        sts_client.get_caller_identity()\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"Invalid AWS credentials: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to validate credentials: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"aws_access_key_id\": credentials.access_key,\n",
    "        \"aws_secret_access_key\": credentials.secret_key,\n",
    "        \"aws_session_token\": credentials.token,\n",
    "        \"region_name\": session.region_name or \"us-east-1\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = load_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from amzn_nova_customization_sdk.dataset.dataset_loader import (\n",
    "    CSVDatasetLoader,\n",
    "    JSONDatasetLoader,\n",
    "    JSONLDatasetLoader,\n",
    ")\n",
    "from amzn_nova_customization_sdk.manager.runtime_manager import (\n",
    "    SMHPRuntimeManager,\n",
    "    SMTJRuntimeManager,\n",
    ")\n",
    "from amzn_nova_customization_sdk.model.model_enums import (\n",
    "    DeployPlatform,\n",
    "    Model,\n",
    "    TrainingMethod,\n",
    ")\n",
    "from amzn_nova_customization_sdk.model.nova_model_customizer import NovaModelCustomizer\n",
    "\n",
    "print(\"‚úÖ SDK imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Your AWS Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these values for your environment\n",
    "S3_BUCKET = \"nova-customization-beta\"  # TODO: Replace with your S3 bucket\n",
    "S3_DATA_PATH = f\"s3://{S3_BUCKET}/demo/input\"\n",
    "S3_OUTPUT_PATH = f\"s3://{S3_BUCKET}/demo/output\"\n",
    "\n",
    "print(f\"Data Path: {S3_DATA_PATH}\")\n",
    "print(f\"Output Path: {S3_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Your Dataset\n",
    "\n",
    "The SDK supports three formats: **JSONL**, **JSON**, and **CSV**. This example uses JSONL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample training data\n",
    "import json\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"answer\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain what AWS is.\",\n",
    "        \"answer\": \"AWS (Amazon Web Services) is a comprehensive cloud computing platform that provides on-demand computing resources and services.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is Python used for?\",\n",
    "        \"answer\": \"Python is a versatile programming language used for web development, data analysis, artificial intelligence, scientific computing, and automation.\",\n",
    "    },\n",
    "] * 100\n",
    "\n",
    "# Save sample data locally\n",
    "with open(\"training_data.jsonl\", \"w\") as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Sample data created: training_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Transform the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset loader\n",
    "loader = JSONLDatasetLoader(\n",
    "    question=\"question\",  # Column name for questions in your data\n",
    "    answer=\"answer\",  # Column name for answers in your data\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "loader.load(\"training_data.jsonl\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nüìä Dataset Preview:\")\n",
    "loader.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data for Nova model training\n",
    "loader.transform(method=TrainingMethod.SFT_LORA, model=Model.NOVA_LITE_2)\n",
    "\n",
    "print(\"‚úÖ Data transformed to Converse format\")\n",
    "print(\"\\nüìä Transformed Data Preview:\")\n",
    "\n",
    "loader.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation sets\n",
    "train_loader, val_loader, _ = loader.split_data(\n",
    "    train_ratio=0.7, val_ratio=0.2, test_ratio=0.1\n",
    ")\n",
    "\n",
    "# Save datasets\n",
    "# For production, upload to S3:\n",
    "train_path = train_loader.save_data(f\"{S3_DATA_PATH}/train.jsonl\")\n",
    "val_path = val_loader.save_data(f\"{S3_DATA_PATH}/val.jsonl\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training data saved to: {train_path}\")\n",
    "print(f\"‚úÖ Validation data saved to: {val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Runtime Infrastructure\n",
    "\n",
    "Choose between:\n",
    "- **SMTJRuntimeManager**: For SageMaker Training Jobs\n",
    "- **SMHPRuntimeManager**: For SageMaker HyperPod clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_customization_sdk.model.model_enums import Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: SageMaker Training Jobs (SMTJ)\n",
    "runtime = SMTJRuntimeManager(\n",
    "    instance_type=\"ml.p5.48xlarge\",  # Choose appropriate instance\n",
    "    instance_count=4,  # Number of instances\n",
    "    # execution_role=\"<your execution role>\",  # TODO: Choose execution role (if different from current role)\n",
    ")\n",
    "\n",
    "platform = Platform.SMTJ\n",
    "\n",
    "print(\"‚úÖ Runtime configured for SageMaker Training Jobs\")\n",
    "print(f\"   Instance Type: {runtime.instance_type}\")\n",
    "print(f\"   Instance Count: {runtime.instance_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: SageMaker HyperPod (if using HyperPod cluster)\n",
    "# Uncomment and configure if using HyperPod:\n",
    "\n",
    "# runtime = SMHPRuntimeManager(\n",
    "#     instance_type=\"ml.p5.48xlarge\",\n",
    "#     instance_count=4,\n",
    "#     cluster_name=\"your-cluster-name\",\n",
    "#     namespace=\"your-namespace\"\n",
    "# )\n",
    "#\n",
    "# platform = Platform.SMHP\n",
    "#\n",
    "# print(\"‚úÖ Runtime configured for SageMaker HyperPod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Nova Model Customizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customizer\n",
    "customizer = NovaModelCustomizer(\n",
    "    model=Model.NOVA_LITE_2,  # Choose your Nova model\n",
    "    method=TrainingMethod.SFT_LORA,  # Training method\n",
    "    infra=runtime,  # Runtime configuration\n",
    "    data_s3_path=train_path,  # Training data path\n",
    "    output_s3_path=S3_OUTPUT_PATH,  # Output path for artifacts\n",
    ")\n",
    "print(\"‚úÖ NovaModelCustomizer initialized\")\n",
    "print(f\"   Model: Nova Lite 2.0\")\n",
    "print(f\"   Method: SFT with LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "training_config = {\n",
    "    \"max_epochs\": 3,  # Number of epochs\n",
    "    \"lr\": 5e-6,  # Learning rate\n",
    "    \"warmup_steps\": 100,  # Warmup steps\n",
    "    \"global_batch_size\": 64,  # Batch size\n",
    "    \"max_length\": 8192,  # Max sequence length\n",
    "}\n",
    "\n",
    "# Start training\n",
    "training_result = customizer.train(\n",
    "    job_name=\"nova-quickstart-training-nova-2\", overrides=training_config\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training job started!\")\n",
    "print(training_result)\n",
    "print(\n",
    "    f\"   üìç Checkpoint URI where the model will be saved: {training_result.model_artifacts.checkpoint_s3_path}\"\n",
    ")\n",
    "print(f\"   üÜî Job ID: {training_result.job_id}\")\n",
    "print(f\"   üìÇ Output Path: {training_result.model_artifacts.output_s3_path}\")\n",
    "\n",
    "# Save job ID for later\n",
    "job_id = training_result.job_id\n",
    "escrow_uri = training_result.model_artifacts.checkpoint_s3_path\n",
    "output_path = training_result.model_artifacts.output_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Monitor Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) While training is ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View recent training logs\n",
    "print(\"üìã Training Logs:\")\n",
    "print(\"=\" * 80)\n",
    "customizer.get_logs(limit=50, start_from_head=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) After Training is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = training_result.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_customization_sdk.monitor.log_monitor import CloudWatchLogMonitor\n",
    "\n",
    "monitor = CloudWatchLogMonitor.from_job_id(job_id=job_id, platform=platform)\n",
    "monitor.show_logs(limit=100, start_from_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 Evaluate the custom Model (After Training Completes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation jobs allow you to test your customized model against pre-set or custom benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these values for your environment\n",
    "S3_BUCKET = S3_BUCKET\n",
    "S3_DATA_PATH = f\"s3://{S3_BUCKET}/demo/input\"\n",
    "S3_OUTPUT_PATH = f\"s3://{S3_BUCKET}/demo/output\"\n",
    "\n",
    "infra = SMTJRuntimeManager(\n",
    "    instance_type=\"ml.p5.48xlarge\",  # Change the instance type if needed (e.g. p5.48xlarge)\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "evaluator = NovaModelCustomizer(\n",
    "    model=Model.NOVA_LITE_2,  # You can also use your trained model here for eval\n",
    "    method=TrainingMethod.EVALUATION,\n",
    "    infra=infra,\n",
    "    data_s3_path=S3_DATA_PATH,  # The data_s3_path is not used in eval job\n",
    "    output_s3_path=S3_OUTPUT_PATH,  # This will be your eval output path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation can be 3 dimensional \n",
    "- Using Public Benchmark to check on Models generalizability is maintained or not.\n",
    "- Using your custom Data to validate models performance on YOUR tasks.\n",
    "- Using LLM As Judge in domains where response quality is hard to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_customization_sdk.recipe_config.eval_config import EvaluationTask\n",
    "\n",
    "mmlu_eval_result = evaluator.evaluate(\n",
    "    job_name=\"eval-test-mmlu\",  # The job name you specified\n",
    "    eval_task=EvaluationTask.MMLU,  # The eval task\n",
    ")\n",
    "\n",
    "byod_eval_result = evaluator.evaluate(\n",
    "    job_name=\"eval-test-byod\",\n",
    "    eval_task=EvaluationTask.GEN_QA,\n",
    "    data_s3_path=\"s3://<data-s3-bucket>/nova-customization/gen_qa.jsonl\",  # TODO: Replace with your data path\n",
    "    # model_path='s3://customer-escrow-<your-model-ckpt-bucket>/your-model-path/' # TODO: Replace with your model path\n",
    "    overrides={\"max_new_tokens\": 2048},\n",
    ")\n",
    "\n",
    "# byom_eval_result = evaluator.evaluate(\n",
    "#     job_name='eval-test-byom',\n",
    "#     eval_task=EvaluationTask.GEN_QA,\n",
    "#     data_s3_path='s3://<your-byom-dataset-bucket>/input/eval/byom/byom_data.jsonl', # TODO: Replace with your dataset\n",
    "#     processor={\n",
    "#         \"lambda_arn\": \"arn:aws:lambda:<region>:<account>:function:<lambda>\" # TODO: Your byom lambda\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# llm_judge_eval_result = evaluator.evaluate(\n",
    "#     job_name='eval-test-llm-judge',\n",
    "#     eval_task=EvaluationTask.LLM_JUDGE,\n",
    "#     data_s3_path='s3://<your-llm-judge-dataset-bucket>/input/eval/llm_judge/llm_judge.jsonl' # TODO: Replace with your dataset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  üìç Bring Your Own Data Job ID: \", byod_eval_result.job_id)\n",
    "print(\"  üìÇ Bring Your Own Data Output Path:\", byod_eval_result.eval_output_path)\n",
    "print(\"  üìç MMLU Job ID:\", mmlu_eval_result.eval_output_path)\n",
    "print(\"  üìÇ MMLU Output Path:\", mmlu_eval_result.eval_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View recent training logs\n",
    "\n",
    "print(\"üìã Evaluation Job Logs:\")\n",
    "print(\"=\" * 80)\n",
    "evaluator.get_logs(limit=50, start_from_head=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Deploy Your Model (After Training Completes)\n",
    "\n",
    "Once training is complete, deploy your model to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the model artifact path from the `s3_output_path`\n",
    "import json\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "# Download and extract manifest from S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = S3_OUTPUT_PATH.split(\"/\")[2]\n",
    "key = f\"{'/'.join(S3_OUTPUT_PATH.split('/')[3:])}/{job_id}/output/output.tar.gz\"\n",
    "\n",
    "print(f\"Bucket is {bucket}, Key is {key}\")\n",
    "\n",
    "with tempfile.NamedTemporaryFile() as tmp_file:\n",
    "    s3.download_file(bucket, key, tmp_file.name)\n",
    "    with tarfile.open(tmp_file.name, \"r:gz\") as tar:\n",
    "        manifest_content = tar.extractfile(\"manifest.json\").read()\n",
    "        manifest = json.loads(manifest_content)\n",
    "\n",
    "model_artifacts_path = manifest[\"checkpoint_s3_bucket\"]\n",
    "print(model_artifacts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model artifacts path from training result\n",
    "# After training completes, use:\n",
    "\n",
    "# Deploy to Bedrock On-Demand\n",
    "deployment_result = customizer.deploy(\n",
    "    model_artifact_path=model_artifacts_path,\n",
    "    deploy_platform=DeployPlatform.BEDROCK_OD,\n",
    "    endpoint_name=\"my-custom-nova-model\",\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Model deployment started!\")\n",
    "print(f\"   Endpoint Name: {deployment_result.endpoint.endpoint_name}\")\n",
    "print(f\"   Status: {deployment_result.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've completed the basic workflow:\n",
    "\n",
    "‚úÖ **Loaded and prepared data** in JSONL format  \n",
    "‚úÖ **Transformed data** to Nova's Converse format  \n",
    "‚úÖ **Configured runtime** infrastructure (SMTJ or HyperPod)  \n",
    "‚úÖ **Started training** with custom hyperparameters  \n",
    "‚úÖ **Monitored progress** via logs  \n",
    "‚úÖ **Deployed the model** to Amazon Bedrock  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Use your own data**: Replace the sample data with your training dataset\n",
    "- **Tune hyperparameters**: Adjust learning rate, batch size, epochs, etc.\n",
    "- **Evaluate performance**: Use `customizer.evaluate()` to benchmark your model\n",
    "- **Run batch inference**: Process large datasets with `customizer.batch_inference()`\n",
    "\n",
    "## Available Models\n",
    "\n",
    "\n",
    "## Available Training Methods\n",
    "\n",
    "- `TrainingMethod.SFT_LORA` - Supervised Fine-Tuning with LoRA\n",
    "- `TrainingMethod.SFT_FULL` - Full supervised fine-tuning\n",
    "- `TrainingMethod.RFT_LORA` - Reinforcement Fine-Tuning with LoRA\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Amazon Nova Models](https://aws.amazon.com/bedrock/nova/)\n",
    "- [SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/)\n",
    "- [Amazon Bedrock](https://aws.amazon.com/bedrock/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
